<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Actions and interventions | Seeing and Doing</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Actions and interventions" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A blog about the foundations of causal inference" />
<meta property="og:description" content="A blog about the foundations of causal inference" />
<link rel="canonical" href="https://davidoj.github.io/causality/03_interventions.html" />
<meta property="og:url" content="https://davidoj.github.io/causality/03_interventions.html" />
<meta property="og:site_name" content="Seeing and Doing" />
<script type="application/ld+json">
{"@type":"WebPage","headline":"Actions and interventions","url":"https://davidoj.github.io/causality/03_interventions.html","description":"A blog about the foundations of causal inference","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="/assets/css/style.css?v=4af449782c34480d5dc9d8cc2c56ecd967547ca4">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Seeing and Doing</h1>
      <h2 class="project-tagline">A blog about the foundations of causal inference</h2>
      
        <a href="https://github.com/davidoj/davidoj.github.io" class="btn">View on GitHub</a>
      
      
    </section>

    <section class="main-content">
      <h1 id="actions-and-interventions">Actions and interventions</h1>

<p>In chapters <a href="/causality/01_three_questions">1</a> and <a href="/causality/02_counterfactuals">2</a> I discussed three questions about causation and some tentative answers:</p>

<ul>
  <li>What does it mean to say something causes something else? <em>Maybe</em> it means that in the closest world where <em>something</em> is different, <em>something else</em> must also be different</li>
  <li>How do we learn that something causes something else? <em>Maybe</em> we can do so by setting up experiments that can approximate closest worlds</li>
  <li>What is the point of learning causal relationships? <em>Maybe</em> it is to decide upon actions and to work out who to blame</li>
</ul>

<p>There’s an alternative set of answers to the first two questions based on <em>interventions</em> rather than <em>closest worlds</em>. As I mentioned in the closing of chapter 2, often when we’re trying to describe the difference between “closest worlds”, it’s very intuitive to talk about someone taking an action that creates the difference. The difference between the bolognese-with-chili world and the bolognese-without-chili world is that <em>someone put chili in the bolognese</em>. The difference between the vaccine world and the placebo world is that <em>someone switched the vials</em>. Closest-world counterfactuals don’t require these actions to explain the difference between worlds, and introducing the idea in these terms is probably a bit misleading, but it’s intuitively very appealing (as with many things covered in the first four chapters, closest-world counterfactuals are a complex topic that I don’t want to get distracted by). More importantly, imagining someone taking an action often fits better with the idea of minimal differences between worlds than imagining other ways that the difference could arise, and minimal differences are required by closes world counterfactuals. If we imagine someone standing at a half-cooked pot of bolognese, ready to add the chili, and then deciding either to add it or to continue cooking without it, both worlds we have imagined are indeed quite similar, differing only as a result of a single decision. Adding a freak gust of wind to one world is in contrast a big change - we’ve forced the weather to be different between the two worlds, which does not really seem to satisfy the idea of a minimal difference.</p>

<p>The “interventionist” approach posits that taking actions is actually the foundational idea of causal relationships, not “minimal differences”: in the interventionist approach, saying “X causes Y” means that if I take an action that makes X happen then Y will happen, but Y would not happen otherwise. Thus we turn the above examples on their head: “chili makes bolognese hot” means <em>if I put chili in the bolognese</em> then it will be hot, and the fact that the bolognese is hot in the chili world is due to this causal relationship and not the other way around as it was in the counterfactual theory.</p>

<p>The interventionist definition makes a lot of sense when we are talking about chili making the bolognese hot, but it is harder to understand what it means if we talk about wind making flags flutter. Is it false to say the wind makes flags flutter because the wind is a natural thing, and no-one took any actions to make it blow? Because of objections like this, the interventionist definition uses “ideal interventions” rather than actual actions taken by actual people. Thus the interventionist definition becomes: “X causes Y” means that an <em>ideal intervention</em> that makes X happen will make Y happen, and Y would not happen otherwise.</p>

<p>What, then, are ideal interventions? I can offer a crude definition as follows: an ideal intervention that brings about “chili in my dinner” is an action that causes chili to be in my dinner that does not directly cause anything else and is not caused by anything else (<a href="https://plato.stanford.edu/entries/causation-mani/#exM1_4">see here for a more careful definition</a>). This seems to make some sense - if I imagine putting chili in the pot and also knocking a glass to the floor, it would be silly to say that the chili caused the glass to break, as this is a side-effect of the chili action, so we should exclude side-effects (“does not directly cause anything else”). Also, if I refrain from adding chili after tasting the sauce and finding it to be already sufficiently hot, I might conclude that the end product turns out hot whether or not I add chili, so I want to exclude the possibility of the action itself being caused by something that could alter the consequences (“is not caused by anything else”).</p>

<p>Given that ideal interventions are supposed to define causation, it is a bit unnerving that we need to invoke causes in order to define interventions in the first place. This forces us to lean on pre-existing causal knowledge in order to make use of the “ideal intervention” approach. For example: Ben says that weather forecasts don’t cause the weather, because an ideal intervention to change weather forecasts would leave the weather unchanged. Alisha responds that of course weather forecasts change the weather, and Ben’s “ideal intervention” is not an ideal intervention at all: not only does it change the weather forecasts, but it also interferes with the forecasts’ natural effect on the weather. While Alisha’s position sounds absurd, the interventional approach allows for people who agree on the facts to disagree on causal relationships if they don’t also agree on the background causal assumptions.</p>

<p>(Similarly, it’s possible to agree on causes but disagree on the implications if you disagree on background causal assumptions)</p>

<p>This presents a tricky issue: if we want to construct any causal model based on ideal interventions, we must also assume a whole lot of background causes and we have to hope that anyone we wish to persuade using our causal model also agrees on these background causes. Usually these assumptions are left implicit, presumably on the judgement that they can easily be agreed upon if the issue ever gets raised. However, I don’t think they can.</p>

<p>Consider this question: “how does someone’s ability to jump affect their success at basketball?”. In the language of ideal interventions: if we changed the height to which someone could jump while changing nothing else, how would their success at basketball change? The height to which someone can jump is determined by the force their leg muscles can exert and the length of their legs. Thus (holding the laws of physics and the local gravitational field constant) it is impossible to change the height to which someone can jump without also changing either the force their leg muscles can exert or the length of their legs, or both. Because we have posed this question in the language of ideal interventions, we are obliged to assume background knowledge about the causes of jump height. The most intuitive assumption, I think, is that leg strength and leg length jointly cause jump height. However, by assumption it is impossible to change jump height without changing one of these quantities, and so with this background assumption we are obliged to accept that our original question refers to something physically impossible.</p>

<p>We don’t even have to settle for physical impossibility. Consider “how does the net worth of a married couple at retirement affect the older partner’s life expectancy?” The net worth of a couple is defined as the sum of the assets of the older partner and the assets of the younger partner. It is <em>logically</em> impossible to change their net worth without changing the value of at least one of their assets. Again, I think the most plausible background assumption is that the assets of each partner cause the net worth of the couple, and as with the basketball example this forces us to accept that our original question refers to a logical impossibility.</p>

<p>I think this is sufficient to establish that we cannot rely on being able to resolve the necessary background assumptions <em>without difficulty</em>, but in case you’re not satisfied let’s consider how we could still try to resolve this problem: in the second example, logical impossibility can be avoided by allowing net worth to cause the assets of at least one of the partners, let’s say the older one. In this case, we either need to accept that the older partner’s assets also cause net worth of the couple (and hence that two variables can mutually cause one another) or that an “ideal intervention” on the older partner’s assests causes precisely the opposite change in the younger partner’s.</p>


      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/davidoj/davidoj.github.io">davidoj.github.io</a> is maintained by <a href="https://github.com/davidoj">davidoj</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>
