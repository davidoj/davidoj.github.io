# A second answer: actions and interventions

In chapters [1](/causality/01_three_questions) and [2](/causality/02_counterfactuals) I discussed three questions about causation and some tentative answers:

 - What does it mean to say something causes something else? *Maybe* it means that in the closest world where *something* is different, *something else* must also be different
 - How do we learn that something causes something else? *Maybe* we can do so by setting up experiments that can approximate closest worlds
 - What is the point of learning causal relationships? *Maybe* it is to decide upon actions and to work out who to blame

There's an alternative set of answers to the first two questions based on *interventions* rather than *closest worlds*. As I mentioned in the closing of chapter 2, often when we're trying to describe the difference between "closest worlds", it's very intuitive to talk about someone taking an action that creates the difference. The difference between the bolognese-with-chili world and the bolognese-without-chili world is that *someone put chili in the bolognese*. The difference between the vaccine world and the placebo world is that *someone switched the vials*. Closest-world counterfactuals don't require these actions to explain the difference between worlds, and introducing the idea in these terms is probably a bit misleading, but it's intuitively very appealing (as with many things covered in the first four chapters, closest-world counterfactuals are a complex topic that I don't want to get distracted by). More importantly, imagining someone taking an action often fits better with the idea of minimal differences between worlds than imagining other ways that the difference could arise, and minimal differences are required by closes world counterfactuals. If we imagine someone standing at a half-cooked pot of bolognese, ready to add the chili, and then deciding either to add it or to continue cooking without it, both worlds we have imagined are indeed quite similar, differing only as a result of a single decision. Adding a freak gust of wind to one world is in contrast a big change - we've forced the weather to be different between the two worlds, which does not really seem to satisfy the idea of a minimal difference.

The "interventionist" approach posits that taking actions is actually the foundational idea of causal relationships, not "minimal differences": in the interventionist approach, saying "X causes Y" means that if I take an action that makes X happen then Y will happen, but Y would not happen otherwise. Thus we turn the above examples on their head: "chili makes bolognese hot" means *if I put chili in the bolognese* then it will be hot, and the fact that the bolognese is hot in the chili world is due to this causal relationship and not the other way around (as it was in the counterfactual theory). 

The interventionist definition makes a lot of sense when we are talking about chili making the bolognese hot, but it is harder to understand what it means if we talk about wind making flags flutter. Is it false to say the wind makes flags flutter because the wind is a natural thing, and no-one took any actions to make it blow? Because of objections like this, the interventionist definition uses "ideal interventions" rather than actual actions taken by actual people. Thus the interventionist definition becomes: "X causes Y" means that an *ideal intervention* that makes X happen will make Y happen, and Y would not happen otherwise.

What, then, are ideal interventions? The unique property of ideal interventions is that they are define *with reference to causal models*. For example, an ideal intervention that brings about "chili in my dinner" is an action that causes chili to be in my dinner without affecting or being caused by any other possible causes of my dinner's being hot (this is a crude definition, see [Woodward's intervention conditions](https://plato.stanford.edu/entries/causation-mani/#exM1_4) for a more careful one). The key here is that while we may be uncertain about some of the effects of an ideal chili-adding intervention, there are other things it is forbidden from changing or being affected by, and it seems like we need rather a lot of causal knowledge to begin with to say which things these are. 

Practically, it also seems to call on a different *kind* of causal knowledge than the kind we've discussed so far. I would like to be able to say that my adding chili to the cooking pot is close enough to an ideal intervention it justifies my claim that chili made my dinner hot. Painstakingly going through every possible thing that might raise or lower the chance of chili ending up in my dinner and verifying that either my addition of the chili isn't correlated with this thing, or it would have mattered if it was is an impossible task. In practice what I do is go through a few important ones and then say the rest probably doesn't matter. Randomisation can help rule out "being affected by causes of hot dinners" without the need to document them all, but it doesn't rule out everything. For example, if smelling chili caused me both to perceive its flavour and to want to add it to my food, then smelling the chili as I add it would violate the conditions of an ideal intervention, even if the decision to add it was randomised. So practically there always seems to be a role for "and the rest doesn't matter" reasoning.

Ideal interventions thus require rather a lot of causal knowledge, as well as some kind of (to my knowledge, unknown) approximating principle. If this is the right way to go about talking about causality then such is life, but it is a bit impractical. At the start of this, I wanted to derive a general measure of the difficulty of a causal inference problem, and this requirement to have a whole lot of knowledge just so that the key building blocks of your problem are well defined makes it unfeasible to say "*this* is the generic form of a causal inference problem", which is a necessary step in proving results about causal inference problems.