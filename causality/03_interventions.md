# Actions and interventions

In chapters [1](/causality/01_three_questions) and [2](/causality/02_counterfactuals) I discussed three questions about causation and some tentative answers:

 - What does it mean to say something causes something else? *Maybe* it means that in the closest world where *something* is different, *something else* must also be different
 - How do we learn that something causes something else? *Maybe* we can do so by setting up experiments that can approximate closest worlds
 - What is the point of learning causal relationships? *Maybe* it is to decide upon actions and to work out who to blame

There's an alternative set of answers to the first two questions based on *interventions* rather than *closest worlds*. As I mentioned in the closing of chapter 2, often when we're trying to describe the difference between "closest worlds", it's very intuitive to talk about someone taking an action that creates the difference. The difference between the bolognese-with-chili world and the bolognese-without-chili world is that *someone put chili in the bolognese*. The difference between the vaccine world and the placebo world is that *someone switched the vials*. Closest-world counterfactuals don't require these actions to explain the difference between worlds, and introducing the idea in these terms is probably a bit misleading, but it's intuitively very appealing (as with many things covered in the first four chapters, closest-world counterfactuals are a complex topic that I don't want to get distracted by). More importantly, imagining someone taking an action often fits better with the idea of minimal differences between worlds than imagining other ways that the difference could arise, and minimal differences are required by closes world counterfactuals. If we imagine someone standing at a half-cooked pot of bolognese, ready to add the chili, and then deciding either to add it or to continue cooking without it, both worlds we have imagined are indeed quite similar, differing only as a result of a single decision. Adding a freak gust of wind to one world is in contrast a big change - we've forced the weather to be different between the two worlds, which does not really seem to satisfy the idea of a minimal difference.

The "interventionist" approach posits that taking actions is actually the foundational idea of causal relationships, not "minimal differences": in the interventionist approach, saying "X causes Y" means that if I take an action that makes X happen then Y will happen, but Y would not happen otherwise. Thus we turn the above examples on their head: "chili makes bolognese hot" means *if I put chili in the bolognese* then it will be hot, and the fact that the bolognese is hot in the chili world is due to this causal relationship and not the other way around as it was in the counterfactual theory. 

The interventionist definition makes a lot of sense when we are talking about chili making the bolognese hot, but it is harder to understand what it means if we talk about wind making flags flutter. Is it false to say the wind makes flags flutter because the wind is a natural thing, and no-one took any actions to make it blow? Because of objections like this, the interventionist definition uses "ideal interventions" rather than actual actions taken by actual people. Thus the interventionist definition becomes: "X causes Y" means that an *ideal intervention* that makes X happen will make Y happen, and Y would not happen otherwise.

What, then, are ideal interventions? I can offer a crude definition as follows: an ideal intervention that brings about "chili in my dinner" is an action that causes chili to be in my dinner that does not directly cause and is not caused by anything else ([see here for a less crude definition](https://plato.stanford.edu/entries/causation-mani/#exM1_4)). This seems to make some sense - if I imagine putting chili in the pot and also knocking a glass to the floor, it would be silly to say that the chili caused the glass to break, as this is a side-effect of the chili action, so we should exclude side-effects ("does not directly cause anything else"). Also, if I refrain from adding chili after tasting the sauce and finding it to be already sufficiently hot, I might conclude that the end product turns out hot whether or not I add chili, so I want to exclude the possibility of the action itself being caused by something that could alter the consequences ("is not caused by anything else").

As the foundation of a definition of causation, however, it leaves a little to be desired: it invokes the idea of causation in setting out what it means for something to cause something else! Try to set aside your intuitions about causes and effects for a moment, and consider the following: Ben says that weather forecasts don't cause the weather, because if he secretly paid off the meteorologists to change some of their forecasts but he found the weather continued to follow its old pattern rather than conform to the new forecasts. Alisha responds that of course weather forecasts change the weather, and Ben's action was simply not an ideal intervention; by paying off the meteorologists he unwittingly interfered with the weather such that it no longer conformed to forecasts. While Alisha's position is absurd, it is absurd because of the background causal claims it makes. If we want to show her claim of forecasts causing weather is false, we can't show this using ideal interventions alone - we need to show that her background causal assumptions are false.

This presents a tricky issue: if we want to construct any causal model based on ideal interventions, we must also assume a whole lot of background causes. Usually these assumptions are left implicit, presumably on the judgement that they can be made explicit without difficulty. I am not so sure that these extra assumptions must be harmless. Where could these assumptions come from? Let's consider some possibilities:

1. Maybe we have a rich set of causal relationships that we "just know". Perhaps, for example, we "just know" that bribing meteorologists can't affect the weather. This appears to contradict [Hume's argument](/causality/01_three_questions) that causal knowledge is *not* knowledge that we "just know", but in fact it's not quite a contradiction. Hume's argument shows that *some* causal relationships must be learned, but not that *all* causal relationships must be learned, so it is still possible that we can start with a large set of background causes we just know, and then learn those that can only be learned. I'm not sure this is possible, because I have trouble coming up with candidates for the background causes - for example, bribing weather forecasters affecting the weather seems like something that could have been true and indeed some [Pinoy Catholics](https://www.mypope.com.ph/eggs-offering-for-good-weather/) appear to believe something similar. Still, I don't know.

2. Maybe there are some basic principles that implies a our required set of background causes. Some candidates are:
 - The output of a random number generator has no causes
 - Causes never operate backwards in time
 - [Minimality and/or faithfulness](https://plato.stanford.edu/entries/causal-models/#MiniFaitCond)
While I've seen the first treated as something like a fundamental principle, the second two are usually considered practically useful principles for inferring causes rather than fundamental definitions, with faithfulness being an especially controversial principle. Furthermore, it is not at all obvious that the joint operation of all three principles is sufficient to yield the needed "basic set of causes".





For the closest worlds approach, not having a measure of world similarity is a big weakness; it's not clear that you even have a theory without it. It's not as obvious how much of a weakness needing a lot of causal knowledge to support the notion of ideal interventions is. This requirement does render the definition circular - causal relationships are defined in terms of interventions which are defined in terms of causal relationships and so on. It is possible to define a causal relationship in terms of *other* causal relationships, so if we are willing to accept some causal knowledge to begin with then we can use the interventional account to define *additional* causal relationships. This "required causal knowledge" is of the ruling out variety - we need to be able to conclude that many potential causal relationships are in fact absent. Maybe we can take for granted that we "just know" that a large enough number of causal relationships are impossible to get our defninition in terms of ideal interventions off the ground. This seems unlikely to me - when I talked about causes we all know in [chapter 1](/causality/01_three_questions) I talked about specific instances of one thing causing another. I didn't say, for example, "we all know wind causes flags to flutter and *nothing else* does so". To begin with, it isn't true (waving flags also makes them flutter), but even if I tried to construct a true statement of this form I could never imagine being highly confident that some phenomenon has absolutely no possible causes that I haven't yet thought of. I can possibly accept *just knowing* that X causes Y, but I can't accept *just knowing* that nothing else causes Y. 

Another approach is that we could possibly rule out many potential causal relationships by introducing some general principles like "causes never operate backwards in time" or "the output of a random number generator has no causes" ([minimality and faithfulness](https://plato.stanford.edu/entries/causal-models/#MiniFaitCond) are more technical "general principles" that allow for the elimination of some causal relationships). Maybe with a careful selection of basic principles we could end up ruling out enough causal relationships that we could say exactly what we mean by "an ideal intervention on X".



Practically, it also seems to call on a different *kind* of causal knowledge than the kind we've discussed so far. I would like to be able to say that my adding chili to the cooking pot is close enough to an ideal intervention it justifies my claim that chili made my dinner hot. Painstakingly going through every possible thing that might raise or lower the chance of chili ending up in my dinner and verifying that either my addition of the chili isn't correlated with this thing, or it would have mattered if it was is an impossible task. In practice what I do is go through a few important ones and then say the rest probably doesn't matter. Randomisation can help rule out "being affected by causes of hot dinners" without the need to document them all, but it doesn't rule out everything. For example, if smelling chili caused me both to perceive its flavour and to want to add it to my food, then smelling the chili as I add it would violate the conditions of an ideal intervention, even if the decision to add it was randomised. There always seems to be a role for "and the rest doesn't matter" reasoning. Note that this isn't a question of inference (which also seems to call for such reasoning) but of ensuring that the notion of "causal relationship" is well defined.

Ideal interventions thus require rather a lot of causal knowledge, as well as some kind of (to my knowledge, unknown) approximating principle. If this is the right way to go about talking about causality then such is life, but it is a bit impractical. At the start of this, I wanted to derive a general measure of the difficulty of a causal inference problem, and this requirement to have a whole lot of knowledge just so that the key building blocks of your problem are well defined makes it unfeasible to say "*this* is the generic form of a causal inference problem", which is a necessary step in proving results about causal inference problems.