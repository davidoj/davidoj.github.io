# Counterfactuals and closest worlds

A recap of [chapter 1](/causality/01_three_questions): 

Three key questions about causes and effects are:
 1. What does it mean to say something causes something else?
 2. How do we learn that something causes something else?
 3. What is the purpose of learning that something causes something else?

We established that if one thing causes another then we should observe that these two things are associated, and that observing associations might be part of the process of learning causal relationships. We also established that there was *something else* to causal relationships beyond association (which Hume called a "necessary connection"), but we couldn't say much about what it was. Finally, learning causes and effects is useful to help us choose which actions to take, and to work out who (or what) to blame for outcomes.

A major focus of philosophical work on causality is on describing precisely down the *something else* that separates causal relationships from associations is. There are two major theories that claim to solve this problem: *closest world counterfactuals* and *ideal interventions*. Both fall short of providing a fully satisfying answer to the question; counterfactuals leave key elements unexplained, and ideal interventions provide a circular explanation of causality. "Practical" theories of causality - meaning those that are invoked when people actually try to infer causal effects from data - tend not to stress the foundations and many practitioners take them for granted entirely. Practical theories provide ways of thinking about causality that have proven to be very useful, even if it's not ultimately clear quite when these ways of thinking are and aren't justified.

In this chapter, I will briefly discuss counterfactuals, and in [the next I address ideal interventions](/causality/03_interventions).

Causation is a relationship between possible worlds
Instead of taking causal relationships as a basic fact – as we do when we represent causal relationships with arrows – we can think of causal relationships as expressing counterfactual relationships. There’s a lot to say about counterfactual relationships, and I’m going to try to get away with saying just a little.
The key idea in counterfactuals is that there are multiple ways an event could play out. Imagine you are in the middle of cooking spaghetti bolognese and have a sliced chili in your hand, ready to add to the pot. From here, imagine two different paths the future could take:
    1. You tip your hand and add the chili to the pot
    2. You set the chili aside
Clearly, in world 1 you would expect that your bolognese would be hot, while in world 2, provided you hadn’t added any other hot ingredients, your bolognese would not be hot. This comparison of “possible worlds” is the basic feature of counterfactual analysis. On the basis of this comparison, we can define the sentence “chili caused the bolognese to be hot” to mean that the bolognese is hot in world 1 but not hot in world 2, and similarly define all other causal relationships in terms of the comparison between different “possible worlds”.
The counterfactual way of thinking can be useful if we are wondering whether two things are causally related – for example, if we want to ask “does the measles vaccine protect against measles?”, we can imagine two worlds - one in which everyone going to the doctor for a vaccine receives it as usual, and another where all the same people go to see their doctor, but all the vaccines were surreptitiously switched for some inert placebo. Imagine that we then compare which individuals contract measles in both worlds. If, for example, the same individuals in both worlds contract measles, we could conclude that the vaccines did not offer resistance. On the other hand, if some individuals contract measles in the placebo world but the same individuals do not contract it in the vaccination world, we can conclude that the vaccination protected them from measles.
On this overview, the counterfactual way of thinking appears to be suited to developing a recipe for asking whether a causal effect is present, while the “causes are arrows” way of thinking is suited to thinking about the consequences of a set of causal relationships we already know to exist. In fact, we find that David Lewis takes a view very much like this one ([Event C causes event E if and only if there is a chain of dependencies running from C to E. (Lewis, 2000)](https://www.jstor.org/stable/2678389?casa_token=QOPVnOHOPEgAAAAA%3ALEODObYoZ6jGkMBtmF7v61lERsPXtMIeqQDtpVE08cYjSkF9tuVUBbFgHCbDXQTMExs0KP3wXrH63OTJEQLzPHfLtbVkeSYexYwAX_8YkphgV4Oy6c86&seq=1#metadata_info_tab_contents)). 
There is a complication, however. While it can be quite useful to imagine multiple worlds in the counterfactual style, “what happens in your imagination” is hardly a rigorous definition of a causal relationship. On this basis I could argue that chili doesn’t cause food to be hot, because that’s what I imagine, and someone else could argue that it does, because that’s what they imagine, and no-one could fault the reasoning of either of us. Just as with “arrows are causes” reasoning, we are in need of a more rigorous definition of counterfactuals.
How do we define causality?
Similarity between worlds
The reason why, in the counterfactual approach, we think of “two parallel worlds” with some small difference between them (e.g. chili in the bolognese, or vaccines vs placebo) is that we want think about changing only the cause we are investigating while keeping everything else as close to the same as possible. To investigate the efficacy of measles vaccines, instead of two parallel worlds, I could imagine something more mundane like dividing a collection of people into two groups, one to receive placebos and another to receive actual vaccines. However, if I do this then there will be many differences between both groups: different people will have different hygiene practices, will come into contact with different social groups and will be exposed to different diseases in different ways. Differences between the two groups could be attributed to something other than the vaccine. What our “parallel worlds” imaginary experiment allows us to do is set up a situation where any differences between the two worlds must be attributable to the vaccine.
It is clear, however, that the vaccine or placebo cannot be the only difference between the two worlds in our experiment. If the vaccine is effective, then they will also differ in terms of who gets measles! For this reason, philosopher David Lewis has proposed that “miminal changes” are the basis of counterfactuals: the two worlds we want to compare are the world in which people are not vaccinated (call it “World V”) and the world (“World P”) that is as similar as possible to World V with the condition that in World P everyone received a placebo instead of a vaccine.
Explaining counterfactuals in terms of the “most similar worlds” is a step towards a definition. It turns out to be rather difficult to strictly define “most similar”, however, and most proposals have serious flaws. The issues are somewhat nuanced, and I will avoid delving into them for the sake of brevity. I have never seen an analysis of “most similar worlds” informing the basis of causal assumptions made in any practical work, and I think I am not being particuarly controversial to claim that an adequate means of measuring of world similarity has not been found.
